[{"content":"令我难忘的老师 *写于暑假结束前4天的一个晚上，记录下自23.8.24进入浙大求学以来令我难忘的老师，有些东西不写下来等忘掉就难以挽回了，但是文笔不好，可能像流水账\n白hh老师 大学计算机基础这么课我选了小白老师，小白老师上课很细致，对计算机底层逻辑讲的很清楚，据说老师会在课件放电影，但是我没福气没体验到hhh。python部分也是看的白老师的讲义自学的（txt格式朴实无华），犹记得第一次配置vscode,花了2小时才搞定，跑通解决八皇后问题的代码也是成就感满满，高中时候学信息学的一塌糊涂，可以说小白老师是我大学计算机的启蒙老师。\n之后一件不愉快的事情，小白负责23年的卷子，按他风格出了一堆考察原理和理解的题目，我们在他班上学习的同学可能比较有优势，毕竟白老师也不喜欢死记硬背的考察。但是很多同学复习方向错了，中招之后在朵朵上开喷。白老师在qq群里很不高兴的反驳\u0026quot;这些都是课上讲过的知识点\u0026quot;，我觉得纯粹是考察方式和还有教学沟通上的问题，硬要说的话谁都没问题。最后总评有93，比较遗憾的是因为作业把填空题做成选择题了 ，被扣了很多分。但是这给了我继续学习算法莫大的信心。\n楼xd老师 楼老师比较难评，我甚至不知道应不应该叫他老师​​(因为讲的太烂了)，对着ppt一顿念，成功锻炼了我的自学能力，这也是我大学第一门几乎全是自学的专业课（画法几何）。但是我感觉楼老师人还是比较温和的，如果单独问具体题目的话讲的还可以。\n林zw老师 林老师个子不是很高，每周二早上上完偏微分就来上林老师的理论力学惹，上课体验很好，但是讲到后面运动学我多花了不少时间来琢磨定理。林老师知道我选了数理方法后，还饶有兴味的讨论一道杆体的建模（偏微分的边界条件），还开玩笑说要不以后给我出一道题做做hhh。\n鲍rh老师\u0026amp;伍老师 如果说哪个课到大学以来花的时间最多，哪个课最让我痛苦，数理方法说第二就没敢说第一的了。由于想学好这门课，我坐在第一排，经常和鲍✌深入交流(难绷)，后来期末考试，鲍✌还问我为什么染了个头。伍老师负责教后面的特殊函数，从这里开始上了波强度，在问一道球函数的题目时，伍老师在钉钉上发了几分钟的语音，直到我真的看明白。然而比较抽象的是，期末考试特殊函数的题目都没做出来。\n","date":"2025-02-10T00:00:00Z","image":"http://localhost:1313/22.jpg","permalink":"http://localhost:1313/p/%E4%BB%A4%E6%88%91%E9%9A%BE%E5%BF%98%E7%9A%84%E8%80%81%E5%B8%88/","title":"令我难忘的老师"},{"content":"评价指标 错误率与精度（一个是错误分类占比，一个是正确分类占比）\n准确率 $P$ 与召回率 $R$ 占比（涉及混淆矩阵）\n$F1-score=2\\times\\frac{P\\times R}{P+R}$，$F1-score$用于平衡$P$和$R$\n$ROC$曲线\n$AUC$\n对数损失:\n$logloss=-\\frac{1}{N}\\sum_{i=1}(y_i log\\pmb{p_i}+(1-y_i)log(1-\\pmb{p_i}))$\n$MAE$：平均绝对误差，不常用，因为导数不连续\n$MSE$:均方误差，对应均方根误差(多了个平方根)\nROC（Receiver Operating Characteristic，受试者工作特征）曲线是机器学习和统计学中用来评估二分类模型性能的重要工具。它主要用于衡量模型在不同阈值下的分类能力。\n通俗解释 假设你在开发一个识别垃圾邮件的模型：\n你的模型会给每封邮件一个垃圾邮件评分（0到1之间的概率值）。 你需要设定一个阈值，比如0.5，如果评分大于0.5，就认为是垃圾邮件，否则是正常邮件。 **问题来了：**不同的阈值会导致不同的错误：\n阈值低（比如0.3） → 可能会把一些正常邮件误判为垃圾邮件（假阳性增多）。 阈值高（比如0.7） → 可能会漏掉一些真正的垃圾邮件（假阴性增多）。 ROC曲线的作用就是：\n考察所有可能的阈值，衡量模型的整体分类能力。 展示真正例率（召回率）与假正例率的权衡，帮助选择合适的阈值。 ROC曲线的轴 横轴（FPR，假正例率）：正常邮件被错判为垃圾邮件的概率。 纵轴（TPR，真正例率/召回率）：垃圾邮件被正确识别出来的概率。 曲线越靠近左上角，模型的分类能力越强！ 一个完全随机的模型，其ROC曲线接近对角线（AUC=0.5），而一个完美模型的曲线会贴近左上角（AUC=1.0）。\n显然$TP$越高越好，$FP$越高越好,所以贴近左上角时是最好的，而贴近$y=x$这条直线，接近随机猜测，最差\n（如果比$y=x$还差，预测取个反就好了，就变成比原模型好的预测了，所以说$y=x$是最差的）\n作用 评估模型的整体性能：通过AUC（曲线下的面积）来衡量模型的好坏。 选择最佳阈值：通过观察ROC曲线，可以找到合适的决策阈值，使误判和漏判达到最佳平衡。 比较不同模型：ROC曲线可以用来直观地比较多个模型的优劣。 总结： ROC曲线是用来衡量二分类模型在不同阈值下的表现，帮助找到最优的分类决策，同时也用于比较不同模型的性能。\nAUC的好处 $ROC$相当于需要排序后，决定阈值来输出分类结果，但是$AUC$不需要选手输入一个设定的分数，比较方便的衡量模型的好坏\n这也可以说是对数损失的好处\n","date":"2025-02-08T00:00:00Z","image":"http://localhost:1313/o.jpg","permalink":"http://localhost:1313/p/kaggle%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","title":"kaggle基础知识"},{"content":"MyGO!!!!! - 「栞」 [00:00.000] 作词 : 織田あすか\n[00:00.455] 作曲 : 藤田淳平\n[00:00.910] 编曲 : 藤田淳平\n[00:01.366] “普通”或者“理所当然”是什么意思呢\n[00:07.017] 用手中这把尺子\n[00:09.788] 根本无法衡量清楚\n[00:12.950] 吐出又吸进的不安\n[00:17.982] 我任凭它摆布，静静站着\n[00:23.237] 笨拙地在原地转圈\n[00:26.263] 逃避着受伤的可能\n[00:30.071] 在现实和笔记本之间来回徘徊，安慰着自己\n[00:40.289] 啊，真是个多么难以生存的世界啊\n[00:46.350] 但是，还是\n[00:49.906] 一切都是，一切都因为我是我\n[00:52.471] 躲躲闪闪、默默哭泣，都是因为我是我\n[00:55.478] 一切都，一切都紧紧抱住\n[00:57.939] 稍微休息一下吧\n[01:00.670] 痛的，痛的，飞走吧\n[01:03.572] 为了不被悲伤吞噬一切\n[01:07.399] 我会是我自己的支持者\n[01:12.867] 路还很长\n[01:16.948] 就算稍微休息一下\n[01:22.383] 也不会有人责怪我\n[01:41.090] 一边看着别人的脸色\n[01:46.819] 一边随波逐流地过着衣食住\n[01:49.592] …我到底想做什么呢\n[01:52.716] 就算如此可怜，毫无出息\n[01:57.695] 我还是改变不了自己\n[02:02.974] 犹豫不决，左右为难\n[02:06.019] 害怕被讨厌\n[02:09.769] 在现实和笔记本之间来回徘徊，激励自己\n[02:19.981] 啊，真是个多么难以生存的世界啊\n[02:26.178] 但是，还是\n[02:29.461] 一切都是，一切都因为我是我\n[02:32.136] 总是担心，总是流泪，都是因为我是我\n[02:34.909] 一切都，一切都紧紧抱住\n[02:37.408] 稍微休息一下吧\n[02:40.456] 痛苦，痛苦，飞走吧\n[02:42.954] 不必总是给一切找意义\n[02:45.494] 不必每件事都赋予它意义\n[02:54.632] 我可以相信吗？【可以】\n[02:59.944] 我可以留在这里吗？【可以】\n[03:05.002] 不要太过努力\n[03:07.599] 当你意识到的时候就好\n[03:10.099] 轻轻地接受自己\n[03:26.749] 一切都是，一切都因为我是我\n[03:29.405] 躲躲闪闪、默默哭泣，都是因为我是我\n[03:32.217] 一切都，一切都紧紧抱住\n[03:34.619] 稍微休息一下吧\n[03:37.704] 痛苦，痛苦，飞走吧\n[03:40.325] 为了不被悲伤吞噬一切\n[03:44.226] 我会是我自己的支持者\n[03:49.627] 只有一次的人生\n[03:53.825] 是否去爱是由我决定\n[03:59.234] 如果是这样的话，那就尽全力吧\n[04:07.771] 啊，语言会如鲜花般闪闪发光\n[04:13.913] 所以， 所以\nMyGO!!!!! - 「栞」 [00:00.000] 作词 : 織田あすか\n[00:00.455] 作曲 : 藤田淳平\n[00:00.910] 编曲 : 藤田淳平\n[00:01.366] “普通”とか”あたりまえ”ってなんだろう\n[00:07.017] 今 手にある物差しでは\n[00:09.788] 全然上手く測れなくって\n[00:12.950] 吐いては また吸い込んだ不安に\n[00:17.982] 僕は為すがまま立ち尽くして\n[00:23.237] 不器用で 空回って\n[00:26.263] 傷つくことから逃げている\n[00:30.071] 現実とノートで行ったり来たり 慰めて\n[00:40.289] あぁ なんて生きづらい世界なんだろう\n[00:46.350] だけど だけど\n[00:49.906] ぜんぶ ぜんぶ 僕だから\n[00:52.471] うじうじ しくしく 僕だから\n[00:55.478] ぜんぶ ぜんぶ 抱きしめて\n[00:57.939] 少し眠ろう\n[01:00.670] 痛いの痛いの飛んでゆけ\n[01:03.572] 悲しみにすべてを奪われないように\n[01:07.399] 僕は 僕の味方でいようよ\n[01:12.867] まだまだ長い道の途中\n[01:16.948] ちょっとくらい休憩したって\n[01:22.383] 誰にも叱られはしないから\n[01:41.090] 人の顔色をうかがいながら\n[01:46.819] 流されるままに衣食住\n[01:49.592] …僕は何がしたいんだろう\n[01:52.716] こんなにも惨めで 情けなくても\n[01:57.695] 僕はまだ自分を変えられそうにない\n[02:02.974] 気弱で 八方美人\n[02:06.019] 嫌われることを恐れている\n[02:09.769] 現実とノートで行ったり来たり 励まして\n[02:19.981] あぁ なんて生きづらい世界なんだろう\n[02:26.178] だけど だけど\n[02:29.461] ぜんぶ ぜんぶ 僕だから\n[02:32.136] いじいじ めそめそ 僕だから\n[02:34.909] ぜんぶ ぜんぶ 抱きしめて\n[02:37.408] 少し眠ろう\n[02:40.456] 痛いの痛いの飛んでゆけ\n[02:42.954] 物事になんでもかんでも\n[02:45.494] 意味を見出さなくたっていいからさ\n[02:54.632] 信じてもいいのかな?【いいよ】\n[02:59.944] ここにいてもいいのかな?【いいよ】\n[03:05.002] 頑張りすぎないように\n[03:07.599] 気づいた時でいい\n[03:10.099] 自分自身を優しく受け止めて\n[03:26.749] ぜんぶ ぜんぶ 僕だから\n[03:29.405] うじうじ しくしく 僕だから\n[03:32.217] ぜんぶ ぜんぶ 抱きしめて\n[03:34.619] 少し眠ろう\n[03:37.704] 痛いの痛いの飛んでゆけ\n[03:40.325] 悲しみにすべてを奪われないように\n[03:44.226] 僕は 僕の味方でいようよ\n[03:49.627] たった一度の僕の人生\n[03:53.825] 愛するかどうかは 僕次第\n[03:59.234] だとしたら精一杯に\n[04:07.771] あぁ 言葉はみずみずしく光るよ\n[04:13.913] だから だから\n","date":"2025-02-03T00:00:00Z","image":"http://localhost:1313/dg.jpg","permalink":"http://localhost:1313/p/%E6%A0%9E/","title":"栞"},{"content":"我的朋友，请你千万不要用自己的秘密和隐私去换取ta的关注 转载，已经过博主许可\n那个女孩，小鹿眼睛的女孩 那个女孩，小鹿眼睛的女孩\n那天，天气不好\n但是我很喜欢她\n我喜欢小鹿眼睛的女孩\n就像不会游泳的人跳入水中\n在她面前，我的喉舌又笨又直\n她的笑……我想听的……\n耳边回荡是她的笑\n她喜欢聊八卦，好奇别人身上的秘密\n她喜欢这些，我喜欢她\n所以我要讲这些给她听\n因为我要看见小鹿眼睛，听见她的笑\n是的……我梦里也能听到\n你知道梦里也能听见笑声吗\n我分享我那些可笑蹩脚的暗恋经历\n没错\n我向现在暗恋的人分享我以前的暗恋日记\n情迷意乱，沉溺……发昏\n我说着那些其他人的秘辛\n她笑了，小鹿眼眉开眼笑\n夏日耀眼，空气浮躁\n我想她，我想同她分享她感兴趣，听到就开心的事情\n……是的……是的，我一直没注意\n当我将自己和盘托出的时候\n她却鲜少说出自己的过往和秘密\n这也是我在摆脱思念苦楚几个月才发现\n最终，曲终人散\n其实没有人散，\n我和她从来没有建立互相的联系\n我是个香蕉皮，果肉被我尽献奉出\n她曾经在我的心里呼风唤雨，填海造陆\n我却挑不起她心里的一个小涟漪\n我是败者吗？\n可是也懂了一些吧\n那天我看见这十二只鸟排练齐整\n就像十二神节阵唤起远古记忆\n倘若它们大显神威把我传回刚认识她的那天\n我还是会走向她say hi\n因为那时候我以为我可以获得爱\n","date":"2025-02-03T00:00:00Z","image":"http://localhost:1313/f.jpg","permalink":"http://localhost:1313/p/%E6%88%91%E7%9A%84%E6%9C%8B%E5%8F%8B%E8%AF%B7%E4%BD%A0%E5%8D%83%E4%B8%87%E4%B8%8D%E8%A6%81%E7%94%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E7%A7%98%E5%AF%86%E5%92%8C%E9%9A%90%E7%A7%81%E5%8E%BB%E6%8D%A2%E5%8F%96ta%E7%9A%84%E5%85%B3%E6%B3%A8/","title":"我的朋友，请你千万不要用自己的秘密和隐私去换取ta的关注"},{"content":"春日影 - CRYCHIC 日文歌词 悴んだ心 ふるえる眼差し\n世界で 僕は ひとりぼっちだった\n散ることしか知らない春は\n毎年 冷たくあしらう\n暗がりの中 一方通行に\nただ ただ 言葉を書き殴って\n期待するだけ むなしいと分かっていても\n救いを求め続けた\n（せつなくて いとおしい）\n今ならば 分かる気がする\n（しあわせで くるおしい）\nあの日泣けなかった僕を\n光は やさしく連れ立つよ\n雲間をぬって きらりきらり\n心満たしては 溢れ\nいつしか頬を きらりきらり\n熱く 熱く濡らしてゆく\n君の手は どうしてこんなにも温かいの?\nねぇお願い\nどうかこのまま 離さないでいて\n縁を結んでは ほどきほどかれ\n誰しもがそれを喜び 悲しみながら\n愛を数えてゆく\n鼓動を確かめるように\n（うれしくて さびしくて）\n今だから 分かる気がした\n（たいせつで こわくって）\nあの日泣けなかった僕を\n光は やさしく抱きしめた\n照らされた世界 咲き誇る大切な人\nあたたかさを知った春は\n僕のため 君のための 涙を流すよ\nあぁ なんて眩しいんだろう\nあぁ なんて美しいんだろう\u0026hellip;\n雲間をぬって きらりきらり\n心満たしては 溢れ\nいつしか頬を きらりきらり\n熱く 熱く濡らしてゆく\n君の手は どうしてこんなにも温かいの?\nねぇお願い\nどうかこのまま 離さないでいて\nずっと ずっと 離さないでいて\n中文翻译 悴萎的心，颤抖的目光\n在这个世界里，我曾孤独一人\n只知道凋零的春天\n每年都冷酷地对待我\n在昏暗中，单行道上\n我只是把话写乱，单纯地期待\n即便知道这样是空虚的\n我依然不断寻求救赎\n（那是令人心碎又深刻的情感）\n现在我想我能明白\n（那是既幸福又痛苦的感觉）\n那天，不能哭泣的我\n光明会温柔地陪伴我\n穿过云层，闪闪发光\n填满心灵，溢出\n不知不觉，脸颊闪闪发光\n温热的泪水开始流淌\n你的手，为什么如此温暖？\n嘿，拜托\n请不要离开我\n我用心系上了缘分，然后解开它\n每个人都因喜悦与悲伤而计数爱\n像是为了确认心跳般\n（既高兴又孤独）\n现在我想我能明白\n（既珍贵又害怕）\n那天，不能哭泣的我\n光明会温柔地拥抱我\n照亮的世界，盛开着我珍视的人\n感受到温暖的春天\n会为我和你流下泪水\n啊，真是耀眼啊\n啊，真是美丽啊\u0026hellip;\n穿过云层，闪闪发光\n填满心灵，溢出\n不知不觉，脸颊闪闪发光\n温热的泪水开始流淌\n你的手，为什么如此温暖？\n嘿，拜托\n请不要离开我\n一直，一直，不要离开我\n","date":"2025-02-02T00:00:00Z","image":"http://localhost:1313/cry.jpg","permalink":"http://localhost:1313/p/%E6%98%A5%E6%97%A5%E5%BD%B1/","title":"春日影"},{"content":"图卷积神经网络 (GCN) 对于一个类似这样的图，记$A_{node_nun\\times node_num}为邻接矩阵$, $A_{A,:}$ 表示了node A的连接方式\n我们用一个图卷积神经对其进行表示学习，比如说对于A，我们设置一个3层的计算图深度，相当于它的视野域可以看到和A连接2层的node\n这里的计算题不能过深，否则会导致各个节点的output趋同，一般2层就够了\n计算图深度不等于神经网络深度，神经网络深度理论上可以无限深\n图卷积计算图公式 $$\rH^{(l+1)}=\\sigma(D^{-\\frac{1}{2}}\\widetilde{A}D^{-\\frac{1}{2}}H^{(l)}W^{(l)})\\ \\ \\ (\\widetilde{A}=A+E_{node\\_num\\times node\\_num})\r$$params:\nD:对角矩阵，表示为每个节点连接的个数，可以理解为一个节点如果连接的节点数过多，难免对这些节点影响是有限的\nW：训练矩阵\nl:第l层\n","date":"2025-02-01T00:00:00Z","image":"http://localhost:1313/m1.jpg","permalink":"http://localhost:1313/p/%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-gcn/","title":"图卷积神经网络  (GCN)"},{"content":"EM算法详细推导 EM算法 step 1.根据目前参数估计期望\nstep 2.最大化似然函数\nstep 3.更新算法\n算法思路 假设结果由k个高斯分布产生，$\\mathcal{N}_i(\\mu_i,\\sigma_i)(i=1,2\u0026hellip;k)$，系数为$\\pi_i(i=1,2\u0026hellip;k)$,结果为$x_j(j=1,2,\u0026hellip;n)$,\n$P(x|\\theta)=\\sum_i a_i\\phi(x|\\theta)$,(其中$\\phi$是高斯分布的密度函数)\n对于n元高斯分布，$f(x)=\\frac{1}{\\sqrt{(2\\pi)^n\\det|\\Sigma|}}\\exp(-\\frac{1}{2}(x^T-\\mu^T)\\Sigma^{-1}(x-\\mu))$\ndet:行列式\n$\\Sigma$:协方阵，对角线为方差，非对角线为协方差，cov(X,Y)表示线性相关程度\n$$\rL(\\theta)=\\sum_jlogP(x_j|\\theta),记Q(z)为隐变量z出现的概率,满足\\sum q(z)=1\\\\改写为L(\\theta)=\\sum_j log\\sum_z P(x_j,z|\\theta) (z含各种情况)=\\sum_j log\\sum_z q(Z) \\frac{ P(x_j,z|\\theta)}{q(z)}\\ge\\sum_j \\sum_zq(Z)log \\frac{ P(x_j,z|\\theta)}{q(z)}\r$$\r由Jensen 不等式：$ f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)] $,f为凹函数\n相当于求$E[\\log\\frac{P(x_j,z|\\theta)}{Q(z)}|\\theta^{{i}}]$\n1. 目标：最大化对数似然函数 $$\rL(\\theta) = \\sum_j \\log P(x_j | \\theta)\r$$$$\rL(\\theta) = \\sum_j \\log \\sum_z P(x_j, z | \\theta)\r$$$$\r\\sum_z q(z) = 1\r$$$$\rL(\\theta) = \\sum_j \\log \\sum_z q(z) \\frac{P(x_j, z | \\theta)}{q(z)}\r$$$$\r\\sum_j \\log \\sum_z q(z) \\frac{P(x_j, z | \\theta)}{q(z)} \\geq \\sum_j \\sum_z q(z) \\log \\frac{P(x_j, z | \\theta)}{q(z)}\r$$$$\rQ(\\theta | \\theta^{(t)}) = \\sum_j \\sum_z q(z) \\log P(x_j, z | \\theta)\r$$ 先验概率 (Prior Probability) 先验概率是指在观察数据或证据之前，根据已有知识或经验对事件发生可能性的主观评估。 例如，在没有任何额外信息的情况下，你认为某人感染某种疾病的可能性是 1%。 后验概率 (Posterior Probability) 后验概率是在观察数据或证据之后，根据新信息对事件发生可能性的更新评估。它是在先验概率的基础上结合新数据计算得出的。 例如，经过医学检测后，结合检测结果重新评估某人感染该疾病的可能性。 2. E 步（Expectation Step） $$\rq(z) = P(z | x, \\theta^{(t)})\r$$$$\rP(z | x, \\theta^{(t)}) = \\frac{P(x | z, \\theta^{(t)}) P(z | \\theta^{(t)})}{P(x | \\theta^{(t)})}\r$$其中：\n$P(z | \\theta^{(t)}) $是隐变量的先验概率 $ P(x | z, \\theta^{(t)}) $ 是在给定隐变量 $ z $的情况下 $ x $ 的概率 $$\rP(x_j, z | \\theta^{(t)}) = P(x_j | z, \\theta^{(t)}) P(z | \\theta^{(t)})\r$$$$\rP(z | x_j, \\theta^{(t)}) = \\frac{P(x_j | z, \\theta^{(t)}) P(z | \\theta^{(t)})}{\\sum_k P(x_j | z_k, \\theta^{(t)}) P(z_k | \\theta^{(t)})}\r$$$$\r\\gamma_{jk} = P(z_k | x_j, \\theta^{(t)}) = \\frac{\\pi_k^{(t)} \\mathcal{N}(x_j | \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{k'} \\pi_{k'}^{(t)} \\mathcal{N}(x_j | \\mu_{k'}^{(t)}, \\Sigma_{k'}^{(t)})}\r$$其中：\n$ \\mathcal{N}(x | \\mu_k, \\Sigma_k) $ 是高斯密度函数 $\\pi_k^{(t)} $ 是第 $ k $ 个高斯分量的权重 $ \\gamma_{jk} $ 是数据点$ x_j $ 属于第$ k $ 个高斯分量的责任度 E步相当于我们已经固定了$q(z)$，所以M步我们不需要考虑$q(z)$，它已经相当于不变量了\n3. M 步（Maximization Step） $$\rQ(\\theta | \\theta^{(t)}) = \\sum_j \\sum_z P(z | x_j, \\theta^{(t)}) \\log P(x_j, z | \\theta)\r$$$$\rP(x_j, z | \\theta) = P(x_j | z, \\theta) P(z | \\theta)\r$$$$\rQ(\\theta | \\theta^{(t)}) = \\sum_j \\sum_k \\gamma_{jk} \\log \\left( P(x_j | z_k, \\theta) P(z_k | \\theta) \\right)\r$$对每个参数进行最大化：\n1. 更新均值 $ \\mu_k $ $$\r\\sum_j P(z_k | x_j, \\theta^{(t)}) \\log P(x_j | z_k, \\theta)\r$$$$\r\\log P(x_j | z_k, \\theta) = -\\frac{1}{2} (x_j - \\mu_k)^T \\Sigma_k^{-1} (x_j - \\mu_k) - \\frac{1}{2} \\log |\\Sigma_k| - \\frac{d}{2} \\log(2\\pi)\r$$$$\rQ(\\mu_k) = \\sum_j P(z_k | x_j, \\theta^{(t)}) \\left( -\\frac{1}{2} (x_j - \\mu_k)^T \\Sigma_k^{-1} (x_j - \\mu_k) \\right)\r$$$$\r\\frac{\\partial Q}{\\partial \\mu_k} = \\sum_j P(z_k | x_j, \\theta^{(t)}) \\Sigma_k^{-1} (x_j - \\mu_k)\r$$$$\r\\sum_j P(z_k | x_j, \\theta^{(t)}) (x_j - \\mu_k) = 0\r$$$$\r\\mu_k^{(t+1)} = \\frac{\\sum_j P(z_k | x_j, \\theta^{(t)}) x_j}{\\sum_j P(z_k | x_j, \\theta^{(t)})}\r$$ 2. 更新协方差矩阵 $\\Sigma_k $ $$\r\\sum_j P(z_k | x_j, \\theta^{(t)}) \\log P(x_j | z_k, \\theta)\r$$$$\rQ(\\Sigma_k) = \\sum_j P(z_k | x_j, \\theta^{(t)}) \\left( -\\frac{1}{2} (x_j - \\mu_k)^T \\Sigma_k^{-1} (x_j - \\mu_k) - \\frac{1}{2} \\log |\\Sigma_k| \\right)\r$$$$\r\\frac{\\partial Q}{\\partial \\Sigma_k} = -\\frac{1}{2} \\sum_j P(z_k | x_j, \\theta^{(t)}) \\left( \\Sigma_k^{-1} (x_j - \\mu_k) (x_j - \\mu_k)^T \\Sigma_k^{-1} - \\Sigma_k^{-1} \\right)\r$$$$\r\\Sigma_k^{(t+1)} = \\frac{\\sum_j P(z_k | x_j, \\theta^{(t)}) (x_j - \\mu_k)(x_j - \\mu_k)^T}{\\sum_j P(z_k | x_j, \\theta^{(t)})}\r$$ 3. 更新混合权重 $ \\pi_k $ $$\r\\sum_j P(z_k | x_j, \\theta^{(t)}) \\log P(z_k | \\theta)\r$$$$\rP(z_k | \\theta) = \\pi_k\r$$$$\r\\sum_j P(z_k | x_j, \\theta^{(t)}) \\log \\pi_k\r$$$$\r\\sum_k \\pi_k = 1\r$$$$\r\\mathcal{L}(\\pi_k, \\lambda) = \\sum_j \\sum_k P(z_k | x_j, \\theta^{(t)}) \\log \\pi_k + \\lambda \\left( \\sum_k \\pi_k - 1 \\right)\r$$$$\r\\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = \\sum_j P(z_k | x_j, \\theta^{(t)}) \\frac{1}{\\pi_k} + \\lambda = 0\r$$$$\r\\pi_k = -\\frac{1}{\\lambda} \\sum_j P(z_k | x_j, \\theta^{(t)})\r$$$$\r\\sum_k \\pi_k = 1\r$$$$\r\\pi_k^{(t+1)} = \\frac{1}{N} \\sum_j P(z_k | x_j, \\theta^{(t)})\r$$ 其中 N 是数据点总数。\n4. 迭代过程 E步: 计算 责任度 $ \\gamma_{jk}$。 M步: 更新 $ \\mu_k, \\Sigma_k, \\pi_k $。 迭代直到收敛，即： $$\r\\|\\theta^{(t+1)} - \\theta^{(t)}\\| \u003c \\epsilon\r$$ 或达到最大迭代次数。 总结 E步 计算后验概率： $$\rP(z_k | x_j, \\theta^{(t)}) = \\frac{\\pi_k^{(t)} \\mathcal{N}(x_j | \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{k'} \\pi_{k'}^{(t)} \\mathcal{N}(x_j | \\mu_{k'}^{(t)}, \\Sigma_{k'}^{(t)})}\r$$ 计算 Q 函数。 M步 更新均值： $$\r\\mu_k^{(t+1)} = \\frac{\\sum_j P(z_k | x_j, \\theta^{(t)}) x_j}{\\sum_j P(z_k | x_j, \\theta^{(t)})}\r$$ 更新协方差： $$\r\\Sigma_k^{(t+1)} = \\frac{\\sum_j P(z_k | x_j, \\theta^{(t)}) (x_j - \\mu_k)(x_j - \\mu_k)^T}{\\sum_j P(z_k | x_j, \\theta^{(t)})}\r$$ 更新混合系数： $$\r\\pi_k^{(t+1)} = \\frac{1}{N} \\sum_j P(z_k | x_j, \\theta^{(t)})\r$$ 这样，EM算法不断更新参数，最终收敛到局部最优解。\n","date":"2025-01-28T00:00:00Z","image":"http://localhost:1313/y.jpg","permalink":"http://localhost:1313/p/em%E7%AE%97%E6%B3%95%E8%AF%A6%E7%BB%86%E6%8E%A8%E5%AF%BC/","title":"EM算法详细推导"},{"content":"常见的矩阵求导公式及推导 1. 标量二次型求导 $$\r\\frac{\\partial x^T A x}{\\partial x} = (A + A^T)x\r$$推导：\n$$\rx^T A x = \\sum_{i} \\sum_{j} x_i A_{ij} x_j\r$$$$\r\\frac{\\partial (x^T A x)}{\\partial x_k} = \\sum_{i} A_{ki} x_i + \\sum_{j} A_{kj} x_j = \\sum_{i} (A + A^T)_{ki} x_i\r$$$$\r\\frac{\\partial x^T A x}{\\partial x} = (A + A^T)x\r$$ 2. 线性项求导 $$\r\\frac{\\partial a^T x}{\\partial x} = a\r$$推导：\n$$\ra^T x = \\sum_{i} a_i x_i\r$$$$\r\\frac{\\partial}{\\partial x_k} \\sum_{i} a_i x_i = a_k\r$$$$\r\\frac{\\partial a^T x}{\\partial x} = a\r$$ 3. 矩阵-向量乘积求导 $$\r\\frac{\\partial Ax}{\\partial x} = A\r$$推导：\n$$\rAx = \\sum_{j} A_{ij} x_j\r$$$$\r\\frac{\\partial}{\\partial x_k} \\sum_{j} A_{ij} x_j = A_{ik}\r$$$$\r\\frac{\\partial Ax}{\\partial x} = A\r$$ 4. 二次型向量偏差求导（对称矩阵） $$\r\\frac{\\partial (x - b)^T A (x - b)}{\\partial x} = 2A(x - b)\r$$ (假设 \\( A \\) 是对称矩阵)\n推导：\n$$\r(x - b)^T A (x - b) = x^T A x - 2 b^T A x + b^T A b\r$$$$\r2 A x - 2 A b\r$$$$\r\\frac{\\partial (x - b)^T A (x - b)}{\\partial x} = 2A(x - b)\r$$ 5. 行列式求导 $$\r\\frac{\\partial \\ln |X|}{\\partial X} = X^{-1}\r$$推导：\n$$\r\\ln |X| = \\text{tr}(\\ln X)\r$$$$\r\\frac{\\partial \\text{tr}(\\ln X)}{\\partial X} = X^{-1}\r$$ 6. 迹的求导 $$\r\\frac{\\partial \\text{tr}(AX)}{\\partial X} = A^T\r$$推导：\n$$\r\\text{tr}(AX) = \\sum_{i} \\sum_{j} A_{ij} X_{ji}\r$$$$\r\\frac{\\partial}{\\partial X_{kl}} \\sum_{i} \\sum_{j} A_{ij} X_{ji} = A_{lk}\r$$$$\r\\frac{\\partial \\text{tr}(AX)}{\\partial X} = A^T\r$$ 7. 逆矩阵求导 $$\r\\frac{\\partial X^{-1}}{\\partial X} = -X^{-1} \\otimes X^{-1}\r$$推导：\n$$\rX X^{-1} = I\r$$$$\r\\frac{\\partial (X X^{-1})}{\\partial X} = 0 \\Rightarrow X^{-1} \\frac{\\partial X}{\\partial X} X^{-1} + \\frac{\\partial X^{-1}}{\\partial X} X = 0\r$$$$\r\\frac{\\partial X^{-1}}{\\partial X} = -X^{-1} \\otimes X^{-1}\r$$ 以上是常见的矩阵求导公式及其简要推导过程。\n","date":"2025-01-26T00:00:00Z","image":"http://localhost:1313/26.jpg","permalink":"http://localhost:1313/p/%E5%B8%B8%E8%A7%81%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E6%B1%82%E5%AF%BC%E5%85%AC%E5%BC%8F/","title":"常见矩阵\u0026向量求导公式"},{"content":"保姆级笔记-详细剖析SMO算法中的知识点 - 知乎\n软间隔支持向量机(soft-margin SVM)详细推导过程 - 知乎\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 import pandas as pd import numpy as np # 数据加载和预处理 data = pd.read_csv(\u0026#34;iris.txt\u0026#34;, header=None) name_list = [\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;, \u0026#34;virginica\u0026#34;] # 标签编码 def safe_index(x, name_list): try: return name_list.index(x) except ValueError: return -1 def data_process(X,y): index=np.arange(0,len(y)) np.random.shuffle(index) return X[index,:],y[index] def split_dataset(X, y, train_ratio=0.8, random_state=None): if random_state is not None: np.random.seed(random_state) n_samples = X.shape[0] indices = np.arange(n_samples) np.random.shuffle(indices) X_shuffled = X[indices] y_shuffled = y[indices] train_size = int(train_ratio * n_samples) X_train, X_predict = X_shuffled[:train_size], X_shuffled[train_size:] y_train, y_predict = y_shuffled[:train_size], y_shuffled[train_size:] return X_train/10, y_train, X_predict/10, y_predict data[4] = data[4].apply(lambda x: safe_index(x, name_list)) data = data[data[4] != -1] # 过滤掉无效数据 print(data) # 提取特征和标签 X = data.iloc[:, :-1].values y = data.iloc[:, -1].values y = np.where(y == 0, -1, 1) # 将标签转换为 -1 和 1 x_train, y_train, x_test, y_test = split_dataset(X, y) class SVM: def __init__(self,x_train,y_train,x_test,y_test,C=1,max_iter=100,tol=0.001): self.X=x_train self.y=y_train self.C=C self.max_iter=max_iter self.a=np.zeros_like(self.y) self.b=0 self.K_matrix=self.compute_k_matrix() self.tol=tol self.X_test=x_test self.y_test=y_test def compute_k_matrix(self): n_samples=self.X.shape[0] K=np.zeros((n_samples,n_samples)) for i in range(n_samples): K[i,:]=np.exp(-np.linalg.norm(self.X[i,:]-self.X,axis=1)) return K def f(self,index): K=self.K_matrix[index,:] return np.sum(self.a*self.y*K)+self.b def choose_index(self): indices=np.where((self.a\u0026gt;0) \u0026amp; (self.a\u0026lt;self.C))[0] if len(indices)==0: indices=np.arange(len(self.y)) index1=np.random.choice(indices) E=np.array([self.compute_E(i) for i in range(len(self.y))]) delta_E=np.abs(E-self.compute_E(index1)) index2=np.argmax(delta_E) return index1,index2 def compute_E(self, index): return self.f(index) - self.y[index] def smo(self): iter_num=0 while iter_num\u0026lt;self.max_iter: max_inner_iter = 100 inner_iter = 0 while inner_iter\u0026lt;max_inner_iter: index1,index2=self.choose_index() a1_old,a2_old=self.a[index1],self.a[index2] y1,y2=self.y[index1],self.y[index2] if y1!=y2: L=max(0,a2_old-a1_old) H=min(self.C,self.C+a2_old-a1_old) else: L = max(0, a1_old + a2_old - self.C) H = min(self.C, a1_old + a2_old) eta=self.K_matrix[index1,index1]+self.K_matrix[index2,index2]-2*self.K_matrix[index1,index2] if eta \u0026lt;= 1e-10: continue a2_new=self.a[index2]+self.y[index2]*(self.compute_E(index1)-self.compute_E(index2))/eta if L == H: continue a2_new=np.clip(a2_new,L,H) a1_new=self.a[index1]+self.y[index1]*self.y[index2]*(self.a[index2]-a2_new) self.a[index1],self.a[index2]=a1_new,a2_new b1 = self.b - self.compute_E(index1) \\ - self.y[index1] * (a1_new - a1_old) * self.K_matrix[index1, index1] \\ - self.y[index2] * (a2_new - a2_old) * self.K_matrix[index1, index2] b2 = self.b - self.compute_E(index2) \\ - self.y[index1] * (a1_new - a1_old) * self.K_matrix[index1, index2] \\ - self.y[index2] * (a2_new - a2_old) * self.K_matrix[index2, index2] if 0 \u0026lt; a1_new \u0026lt; self.C: self.b = b1 elif 0 \u0026lt; a2_new \u0026lt; self.C: self.b = b2 else: self.b = (b1 + b2) / 2 inner_iter+=1 if np.abs(a1_new - a1_old)\u0026lt;self.tol and np.abs(a2_new - a2_old)\u0026lt;self.tol: break if iter_num%10==0: print(f\u0026#34;第{iter_num}轮,预测准确率为{round(self.calculate_accuracy(), 3)}\u0026#34;) print(f\u0026#34;L:{L},H:{H},eta:{eta}\u0026#34;) print(f\u0026#34;a[:20]:{self.a[:20]},b:{self.b}\u0026#34;) iter_num+=1 def predict(self,X): K=np.exp(-np.linalg.norm(self.X-X,axis=1)) if np.sum(self.a*self.y*K)+self.b\u0026gt;=0: return 1 else: return -1 def calculate_accuracy(self): correct_predictions = 0 n_samples = len(self.y_test) for i in range(n_samples): y_pred = self.predict(self.X_test[i]) # 对单个样本预测 if y_pred == y_test[i]: correct_predictions += 1 accuracy = correct_predictions / n_samples return accuracy if __name__==\u0026#34;__main__\u0026#34;: print(x_train) svm=SVM(x_train,y_train,x_test,y_test) svm.smo() y_pred=np.zeros_like(y_test) for i in range(len(svm.X_test)): y_pred[i]=svm.predict(svm.X_test[i,:]) print(f\u0026#34;预测值{y_pred}\u0026#34;) print(f\u0026#34;真实值{y_test}\u0026#34;) ","date":"2025-01-26T00:00:00Z","image":"http://localhost:1313/xz.jpg","permalink":"http://localhost:1313/p/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%8F%8Asmo/","title":"支持向量机及SMO"},{"content":"\nK均值聚类（K-Means）的数学原理 K均值聚类是一种迭代优化算法，其目标是将数据分为 \\( k \\) 个簇，最小化簇内样本点与簇中心的平方误差和（即误差平方和，SSE）。以下是核心数学原理：\n1. 目标函数 K均值算法的目标是最小化以下目标函数： $ J = \\sum_{i=1}^k \\sum_{x \\in C_i} ||x - \\mu_i||^2 $\n\\( k \\)：簇的数量（预定义）。 \\( $C_i $)：第 \\( i \\) 个簇。 \\($ \\mu_i $ \\)：第 \\( i \\) 个簇的中心（质心）。 \\( x \\)：数据点。 \\($||x - \\mu_i||^2$ \\)：数据点 \\( x \\) 与簇中心 \\( \\mu_i \\) 的欧氏距离的平方。 2. 算法流程 K均值通过以下步骤进行迭代优化：\n初始化簇中心： 随机选择 \\( k \\) 个数据点作为初始簇中心，或者随机初始化。\n分配样本点： 对每个样本点 \\( x \\)，计算它到所有簇中心 \\( \\mu_i \\) 的欧氏距离，分配到最近的簇： $ C_i = {x | ||x - \\mu_i||^2 \\leq ||x - \\mu_j||^2, \\forall j \\neq i} $\n更新簇中心： 对每个簇 \\( C_i \\)，重新计算其簇中心为簇中所有点的均值： $ \\mu_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} x $\n迭代： 重复步骤 2 和 3，直到簇中心不再发生变化或达到最大迭代次数。\n3. 收敛性 收敛条件：目标函数 \\( J \\) 收敛到局部最小值。 缺点：对初始值敏感，可能陷入局部最优。 伪代码 以下是 K 均值算法的伪代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 输入: 数据集 X = {x1, x2, ..., xn}，簇的数量 k，最大迭代次数 max_iters 输出: 簇分配结果 C = {C1, C2, ..., Ck} 和簇中心 μ = {μ1, μ2, ..., μk} 1. 初始化 μ = {μ1, μ2, ..., μk} （随机选择 k 个点作为初始簇中心） 2. 重复以下步骤直到收敛或达到最大迭代次数： 2.1 分配样本到最近的簇： 对于每个点 x ∈ X： 计算 d(x, μi) = ||x - μi||^2，针对所有 i 将点 x 分配到距离最近的簇 Ci 2.2 更新簇中心： 对于每个簇 Ci： μi = (1 / |Ci|) * ∑(x ∈ Ci) x 2.3 计算目标函数 J 3. 输出簇分配结果 C 和簇中心 μ 层次聚类的数学原理 层次聚类（Hierarchical Clustering）是一种基于层次结构的聚类方法，分为两种类型：凝聚式层次聚类（Agglomerative Hierarchical Clustering） 和 分裂式层次聚类（Divisive Hierarchical Clustering）。常用的凝聚式层次聚类从单个点开始，将其逐步合并为簇。\n1. 数学原理 层次聚类通过定义簇之间的相似度或距离逐步构建一个层次结构（如树状结构）。核心数学原理包括以下部分：\n1.1 数据点之间的距离 常见的距离度量方式：\n欧氏距离（Euclidean Distance）： $ d(x_i, x_j) = \\sqrt{\\sum_{k=1}^m (x_{ik} - x_{jk})^2} $ 曼哈顿距离（Manhattan Distance）： $ d(x_i, x_j) = \\sum_{k=1}^m |x_{ik} - x_{jk}| $ 1.2 簇之间的距离 定义两个簇 \\( C_i \\) 和 \\( C_j \\) 的距离的方法：\n单链接法（Single Linkage）： $ D(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j} d(x, y) $ 全链接法（Complete Linkage）： $ D(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j} d(x, y) $ 平均链接法（Average Linkage）： $ D(C_i, C_j) = \\frac{1}{|C_i| |C_j|} \\sum_{x \\in C_i, y \\in C_j} d(x, y) $ 质心法（Centroid Linkage）： $ D(C_i, C_j) = ||\\mu_i - \\mu_j||_2 $ 其中 \\( $\\mu_i$ \\) 和 \\( $\\mu_j $\\) 是两个簇的质心。 1.3 层次结构 在凝聚式层次聚类中，初始状态将每个点作为一个簇，逐步合并最近的簇，直到最终所有点合并为一个簇。 在分裂式层次聚类中，从整体簇开始，逐步分裂为多个小簇。 2. 算法流程（以凝聚式为例） 初始化：将每个数据点作为一个独立的簇。 计算所有簇之间的距离，找到最近的两个簇。 合并最近的两个簇。 更新距离矩阵（根据所选的距离计算方法）。 重复步骤 2 至 4，直到所有簇合并为一个簇或达到指定层次。 伪代码 以下是凝聚式层次聚类的伪代码：\n1 2 3 4 5 6 7 8 9 10 11 12 输入: 数据集 X = {x1, x2, ..., xn}，距离度量方式 dist_method 输出: 层次树状结构 Dendrogram 1. 初始化：将每个点 xi 视为一个簇 Ci 2. 计算初始距离矩阵 D，其中 D(i, j) = dist_method(Ci, Cj) 3. 重复以下步骤直到所有簇合并为一个： 3.1 找到距离矩阵 D 中最小的非对角元素，对应的两个簇 Ci 和 Cj 3.2 合并簇 Ci 和 Cj 为新簇 Cnew 3.3 更新距离矩阵 D： 对于每个簇 Ck ≠ Cnew，计算 D(Cnew, Ck) 3.4 删除簇 Ci 和 Cj，添加簇 Cnew 4. 输出层次树状结构 Dendrogram 高斯混合模型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 import numpy as np import matplotlib.pyplot as plt from scipy.stats import multivariate_normal red_mean=2 red_std=3 red=np.random.normal(red_mean,red_std,1000) blue_mean=7 blue_std=1 blue=np.random.normal(blue_mean,blue_std,1000) both_color=np.sort(np.concatenate((red,blue))) #print(both_color) plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,2) # 绘制红色和蓝色点 plt.scatter(red,np.zeros_like(red),color=\u0026#34;red\u0026#34;,s=10) plt.scatter(blue,np.zeros_like(blue),color=\u0026#34;blue\u0026#34;,s=10) plt.title(r\u0026#39;Distribution of red and blue points (known colours)\u0026#39;, fontsize=17) #隐藏y坐标 plt.yticks([]) plt.show() x_train=both_color.reshape(-1,1) class Gaussian: def __init__(self,x_train,num=2,eps=0.1): if len(x_train.shape)==1: self.x_train=x_train.reshape(-1,1) else: self.x_train=x_train self.m=self.x_train.shape[0] self.n=self.x_train.shape[1] self.num=num self.sigma=np.ones((self.num,self.n)) self.mean=np.zeros((self.num,self.n)) self.eps=eps self.resp_val=np.zeros((self.m,self.num)) self.pi=np.ones(self.num)/self.num def train(self): sigma_new = np.inf * np.ones((self.num, self.n)) mean_new = np.inf * np.ones((self.num, self.n)) pi_new = np.inf * np.ones(self.num) m1 = np.column_stack((sigma_new, mean_new, pi_new)) m2 = np.column_stack((self.sigma, self.mean, self.pi)) while np.max(np.abs(m1 - m2)) \u0026gt; self.eps: self.resp_val = self.calc_resp_val() val_sum = np.sum(self.resp_val, axis=0) mean_new = np.dot(self.resp_val.T, self.x_train) / val_sum[:, None] sigma_new = np.zeros((self.num, self.n)) for k in range(self.num): diff = self.x_train - mean_new[k, :] sigma_new[k, :] = np.sum(self.resp_val[:, k,None] * (diff ** 2), axis=0) / val_sum[k] pi_new = val_sum / self.m m1 = np.column_stack((sigma_new, mean_new, pi_new)) m2 = np.column_stack((self.sigma, self.mean, self.pi)) self.mean = mean_new self.sigma = sigma_new self.pi = pi_new print(\u0026#34;mu为:\u0026#34;) print(self.mean) print(\u0026#34;sigma:\u0026#34;) print(self.sigma) print(\u0026#34;pi:\u0026#34;) print(self.pi) def calc_resp_val(self): \u0026#34;\u0026#34;\u0026#34; param: resp_val[j,k]为gamma_j_k \u0026#34;\u0026#34;\u0026#34; resp_val=np.zeros((self.m,self.num)) for i in range(self.m): x=self.x_train[i,:] t=np.array([self.gauss(x,k) for k in range(self.num)]) t=self.pi*t t=t/np.sum(t) resp_val[i,:]=t return resp_val def gauss(self,x,k): cov=np.diag(self.sigma[k,:]) mean=self.mean[k,:] multi_gaussian=multivariate_normal(mean,cov) return multi_gaussian.pdf(x) if __name__==\u0026#34;__main__\u0026#34;: Gaussian=Gaussian(x_train=both_color,num=2,eps=0.1) Gaussian.train() 代码说明 这段代码实现了一个简单的高斯混合模型（Gaussian Mixture Model, GMM）的训练过程，以下是其功能说明：\n代码功能 生成数据： 两组高斯分布数据 red 和 blue 分别从 $N(2,32)\\mathcal{N}(2, 3^2)N(2,32)$ 和$ N(7,12)\\mathcal{N}(7, 1^2)N(7,12)$ 生成，每组 1000 个样本。 合并两组数据形成一个数据集 both_color，用于 GMM 模型的训练。 数据可视化： 使用散点图展示红色和蓝色数据点，红色点和蓝色点分布在两个高斯分布的中心附近，用来直观表示两组数据的分布。 实现高斯混合模型： 实现了一个 Gaussian 类，用于对输入数据训练高斯混合模型（GMM），并估计数据点的混合成分（责任度）。 高斯混合模型训练逻辑： 使用 EM（Expectation-Maximization）算法，包含以下步骤： E 步（Expectation Step）：计算每个数据点属于每个高斯分量的责任度 $\\gamma_{jk}$。 M 步（Maximization Step）：根据计算出的责任度更新均值 $\\mu_k$、协方差 $\\sigma_k^2$ 和混合系数 $\\pi_k$。 模型通过循环迭代，直到参数收敛为止（满足 $\\epsilon$ 条件）。 输出结果： 打印最终训练得到的参数： $\\mu$（每个分量的均值向量） $\\sigma$（每个分量的协方差矩阵） $\\pi$（每个分量的混合系数） 类和方法功能 Gaussian 类： 用于实现高斯混合模型的训练。 接收输入数据、分量数（num）和收敛阈值（eps）。 存储模型的参数（均值、协方差、混合系数）及中间计算结果（责任度）。 train 方法： 执行 EM 算法，包括： 计算数据点的责任度。 更新高斯分布的参数：均值 $\\mu_k$、协方差 $\\sigma_k^2$、混合系数 $\\pi_k$。 打印训练后的参数。 calc_resp_val 方法： 计算每个数据点属于每个高斯分量的责任度 $\\gamma_{jk}$。 gauss 方法： 计算多元正态分布的概率密度值，用于责任度的计算。 执行逻辑 生成两组高斯分布数据，合并并转换为训练数据 x_train。 初始化 Gaussian 对象，设置分量数（num=2）和收敛阈值（eps=0.1）。 调用 train 方法，训练高斯混合模型。 打印模型参数（均值、协方差、混合系数）。 ","date":"2025-01-25T00:00:00Z","image":"http://localhost:1313/25.jpg","permalink":"http://localhost:1313/p/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/","title":"聚类算法"},{"content":"一文读懂循环神经网络(RNN) - 知乎\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 import numpy as np import matplotlib.pyplot as plt # 时间序列 t = np.linspace(0, 10, 500) # 0 到 10 秒，共 500 个点 # 生成波形 sine_wave = np.sin(2 * np.pi * t) # 正弦波 square_wave = np.sign(np.sin(2 * np.pi * t)) # 方波 triangle_wave = 2 * np.abs(2 * (t % 1) - 1) - 1 # 三角波 # 添加噪声 noise = np.random.normal(0, 0.1, sine_wave.shape) # 高斯噪声 sine_wave_noisy = sine_wave + noise square_wave_noisy = square_wave + noise triangle_wave_noisy = triangle_wave + noise # 可视化，三个波形并排显示 fig, axs = plt.subplots(1, 3, figsize=(18, 5)) # 正弦波 axs[0].plot(t, sine_wave, label=\u0026#34;Original Sine Wave\u0026#34;) axs[0].plot(t, sine_wave_noisy, label=\u0026#34;Noisy Sine Wave\u0026#34;, alpha=0.7) axs[0].set_title(\u0026#34;Sine Wave with Noise\u0026#34;) axs[0].legend() # 方波 axs[1].plot(t, square_wave, label=\u0026#34;Original Square Wave\u0026#34;) axs[1].plot(t, square_wave_noisy, label=\u0026#34;Noisy Square Wave\u0026#34;, alpha=0.7) axs[1].set_title(\u0026#34;Square Wave with Noise\u0026#34;) axs[1].legend() # 三角波 axs[2].plot(t, triangle_wave, label=\u0026#34;Original Triangle Wave\u0026#34;) axs[2].plot(t, triangle_wave_noisy, label=\u0026#34;Noisy Triangle Wave\u0026#34;, alpha=0.7) axs[2].set_title(\u0026#34;Triangle Wave with Noise\u0026#34;) axs[2].legend() plt.tight_layout() plt.show() class RNN: def __init__(self, input_size, x_train, y_train, lr=0.01, max_iter=100): self.input_size = input_size self.x_train = x_train self.y_train = y_train self.y_cut_off = self.y_train[input_size:] self.wh = np.random.normal(0, 0.1, input_size-1) self.bh = np.random.normal(0, 0.1, input_size-1) self.wx = np.random.normal(0, 0.1, input_size) self.v = np.random.normal(0, 0.1) self.h = np.zeros(input_size) self.z = np.zeros(input_size) self.lr = lr self.max_iter = max_iter def train(self): iter_num = 0 losses = [] while iter_num \u0026lt; self.max_iter: y_predict = np.zeros(len(self.x_train) - self.input_size) for i in range(0, len(self.x_train) - self.input_size): x = self.x_train[i:i+self.input_size] self.z = self.wx * x self.h[0] = self.sig(self.z[0]) for j in range(1, self.input_size): self.z[j] += self.h[j-1] * self.wh[j-1] + self.bh[j-1] self.h[j] = self.sig(self.z[j]) y_hat = self.v * self.h[-1] y_predict[i] = y_hat y = self.y_cut_off[i] self.back_propagation(x, y_hat, y) loss = self.loss(y_predict) losses.append(loss) print(f\u0026#34;第{iter_num+1}轮训练，MSE损失为{loss}\u0026#34;) iter_num += 1 return y_predict, losses def sig(self, x): return 1 / (1 + np.exp(-x)) def back_propagation(self, x, y_hat, y): dloss_dhi = np.zeros(self.input_size) dloss_dhi[-1] = (y_hat - y) * self.v for i in range(self.input_size - 2, -1, -1): dloss_dhi[i] = dloss_dhi[i + 1] * self.wh[i] * self.sig(self.z[i]) * (1 - self.sig(self.z[i])) dloss_dwh = dloss_dhi[1:] * self.sig(self.z[1:]) * (1 - self.sig(self.z[1:])) * self.h[:-1] dloss_dbh = dloss_dhi[1:] * self.sig(self.z[1:]) * (1 - self.sig(self.z[1:])) dloss_dwx = dloss_dhi * self.sig(self.z) * (1 - self.sig(self.z)) * x dloss_dv = (y_hat - y) * self.h[-1] self.wh -= self.lr * dloss_dwh self.bh -= self.lr * dloss_dbh self.wx -= self.lr * dloss_dwx self.v -= self.lr * dloss_dv def loss(self, y_predict): return 0.5 * np.sum((y_predict - self.y_cut_off) ** 2) # 生成数据 t = np.linspace(0, 100, 200) sine_wave = np.sin(40 * np.pi * t)/1000 # 创建并训练模型 r = RNN(10, t, sine_wave, lr=0.1, max_iter=1000) y_pred, losses = r.train() # 绘制损失曲线 plt.figure(figsize=(12, 5)) plt.plot(losses) plt.title(\u0026#34;Loss Curve\u0026#34;) plt.xlabel(\u0026#34;Iteration\u0026#34;) plt.ylabel(\u0026#34;MSE Loss\u0026#34;) plt.show() # 绘制预测结果 plt.figure(figsize=(12, 5)) plt.plot(t[10:], r.y_cut_off, label=\u0026#34;True\u0026#34;) plt.plot(t[10:], y_pred, label=\u0026#34;Predicted\u0026#34;) plt.title(\u0026#34;Prediction Results\u0026#34;) plt.xlabel(\u0026#34;Time\u0026#34;) plt.ylabel(\u0026#34;Amplitude\u0026#34;) plt.legend() plt.show() ","date":"2025-01-24T00:00:00Z","image":"http://localhost:1313/RNN.jpg","permalink":"http://localhost:1313/p/rnn/","title":"RNN"},{"content":" 这是目前我最喜欢的歌，相比于专辑里的record，我更喜欢不插电的版本：unplugged\n这首歌的解释有多种，我就随便谈谈我的感受吧。当一首歌写完，从某种意义上来说，已经不只是作者的作品了，每个人都有自己的独特经历，也就有了不一样的感受。\n一种解释是，男主的爱人去世了，人们手捧鲜花，把坟墓上的石头”放回原位“，当他们开始铲土，埋葬她时，作者觉得自己已经随着她死去了。“埋葬在这子宫里”，这座坟墓既凄冷，但是又生机盎然——因为它埋葬了作者的爱。“他觉得坟墓其实就是子宫。和她葬在一起，他觉得自己可以绽放，他们可以绽放\u0026hellip;\u0026hellip;就像种下的种子”。\n在我心里，这也是一首对我有特殊意义的歌曲，在我搞工高班的时候，看着超出我能力的逆天题目，和日常的学习压力，只能给自己加压，“我前进的太远，以至于忘了为何出发”。在我看来，这是一个放弃了自己的人。第一句歌词，“请把我温柔的埋葬在子宫里”，其实就是自杀的隐喻，人来于子宫，但是从他出生开始，就被压力和压抑包围着，“子宫”其实就是失去安全感和绝望的具象化吧，当你无路可走，就想回到一个安全温暖的母体里。\n男主失去了世界上唯一能够理解他的人，也就彻底失去了活下去的希望，他不愿意再和这个世界交流，他自卑而敏感，他渴望飞翔，渴望逃离，但是“翅膀早就被折断了”。“我无法成为你们期待的人”，这里更多的是男主对自己的失望和绝望，自我否定，自我怀疑，他和世界格格不入，无地自容。\n我认为最有意思的是这句歌词：“我吃掉了太阳，我的舌头上有着灼伤的味道”。当一个人想到自己爱慕的人，除了爱意，可能还有对自己的自卑和对未来的迷茫吧。她就是你的太阳，你享受着她的光芒，但是你的舌头也被“灼伤了”。在这里，作者回忆着和爱人的点点滴滴，但是太阳的光芒，在这时变成了痛苦，也就被”灼伤了“。\n也许这首歌里的爱人，从来就是男主的臆想，“她”只是男主的想象，为了不那么孤独。但是这些都不重要，随着羽翼的断裂，男主坠入深渊，也就彻底放弃了自己。\ndown in a hole\nBury me softly in this womb\n请把我温柔的埋葬在子宫里\nI give this part of me for you\n我把我的一部分给予你\nSand rains down and here I sit\n沙雨落下，我在这静静矗立\nHolding rare flowers in a tomb in bloom\n在她的坟前，手捧珍稀的鲜花，在坟墓里，绽放\u0026hellip;\nDown in a hole and I don\u0026rsquo;t know if I can be saved\n坠入深渊，有人来救救我吗？\nSee my heart I decorate it like a grave\n看着我的心啊，它被我点缀成一座坟墓\nYou don\u0026rsquo;t understand who they thought I was supposed to be\n我无法成为你们期待的人\nLook at me now a man who won\u0026rsquo;t let himself be\n看着这个人啊，他无地自容\nDown in a hole, feelin\u0026rsquo; so small\n坠入深渊，感到如此渺小\u0026hellip;\nDown in a hole, losin\u0026rsquo; my soul\n坠入深渊，失去我的灵魂,\u0026hellip;\nI\u0026rsquo;d like to fly, but my wings have been so denied\n我想要飞翔，但是我的翅膀早就被折断了\u0026hellip;\nDown in a hole and they\u0026rsquo;ve put all the stones in their place\n坠入深渊，他们把石头放回原位\nI\u0026rsquo;ve eaten the sun so my tongue has been burned of the taste\n我吃掉了太阳，我的舌头上有着灼伤的味道\nI have been guilty of kicking myself in the teeth\n在自责的痛苦里，不能自已\nI will speak no more of my feelings beneath\n我不会在说出我心底的感受了\nDown in a hole, feelin\u0026rsquo; so small\n坠入深渊，感到如此渺小\u0026hellip;\nDown in a hole, losin\u0026rsquo; my soul\n坠入深渊，失去我的灵魂,\u0026hellip;\nI\u0026rsquo;d like to fly but my wings have been so denied\n我想要飞翔，但是我的翅膀早就被折断了\u0026hellip;\nBury me softly in this womb\n请把我温柔的埋葬在子宫里\nOh I want to be inside of you\n我想和你融为一体\nI give this part of me for you\n我把我的一部分给予你\nOh I want to be inside of you\n我想和你融为一体\nSand rains down and here I sit\n沙雨落下，我在这静静矗立\nHolding rare flowers in a tomb in bloom (Oh I want to be inside of you)\n在她的坟前，手捧珍稀的鲜花，在坟墓里，绽放\u0026hellip;\nOh I want to be inside\n我想和你融为一体\nDown in a hole, feelin\u0026rsquo; so small\n坠入深渊，感到如此渺小\u0026hellip;\nDown in a hole, losin\u0026rsquo; my soul\n坠入深渊，失去我的灵魂,\u0026hellip;\nDown in a hole, feelin\u0026rsquo; so small\n坠入深渊，感到如此渺小\u0026hellip;\nDown in a hole, outta control\n坠入深渊，失去控制\u0026hellip;\nI\u0026rsquo;d like to fly but my wings have been so denied\n我想要飞翔，但是我的翅膀早就被折断了\u0026hellip;\n","date":"2025-01-23T00:00:00Z","image":"http://localhost:1313/jg.png","permalink":"http://localhost:1313/p/down-in-a-hole/","title":"down in a hole"},{"content":"​\n一阶线性方程 可以直接分离变量 齐次的形式（$\\frac{\\mathrm{d}y}{\\mathrm{d}x}=f(\\frac{y}{x})$） 一阶线性非齐次方程 （$\\frac{\\mathrm{d} y}{\\mathrm{d} x}+P(x)y=Q(x)$） 伯努利方程（$\\frac{\\mathrm{d} y}{\\mathrm{d} x}+P(x)y=Q(x)y^{n}$） $$\r令u=\\frac{y}{x},那么y=ux,\\frac{\\mathrm{d}y}{\\mathrm{d}x}=\\frac{\\mathrm{d}(ux)}{\\mathrm{d}x}=u+x\\frac{\\mathrm{d}u}{\\mathrm{d}x}\r$$$$\r\\frac{\\mathrm{d}y}{y}=-P\\cdot \\mathrm{d}x\r$$$$\r\\Leftrightarrow y=Ce^{-\\int P dx}\r$$​\t(ii)Q$\\ne$0 ，使用常数变异法$u=u(x)$,我们用它代替C\n$$\r于是,\\frac{\\mathrm{d}u}{\\mathrm{d}x}=Qe^{\\int P \\mathrm{d}x},u=\\int Qe^{\\int P \\mathrm{d}x} \\mathrm{d}x+C'\r$$$$\r带入得，y=e^{-\\int P dx} \\cdot(\\int Qe^{\\int P \\mathrm{d}x} \\mathrm{d}x+C')\r$$第四个，伯努利方程，将左右都除以$y^{n}$就转化为了第三种形式\n$$\ry^{-n}\\frac{\\mathrm{d} y}{\\mathrm{d} x}+P(x)y^{1-n}=Q(x)\r$$$$\r\\Leftrightarrow \\frac{1}{1-n}\\frac{\\mathrm{d} y^{1-n}}{\\mathrm{d} x}+P(x)y^{1-n}=Q(x),再令z=y^{1-n}\r$$ 全微分方程 $$\r求：xy(y-xy')=x+yy',y(0)=\\frac{\\sqrt{2}}{2}\r$$$$\r\\Leftrightarrow y'=\\frac{xy^{2}-x}{x^{2}y+y}，显然不是前面的形式\r$$$$\r\\Leftrightarrow (xy^{2}-x)\\mathrm{d}x-(x^{2}y+y)\\mathrm{d}y=0\r$$$$\r\\frac{\\partial P}{\\partial y}=2xy,\\frac{\\partial Q}{\\partial x}=2xy,确认为全微分方程\r$$$$\rC=-\\int_{0}^{y}y\\mathrm{d}y+\\int_{0}^{x}(xy^{2}-x)\\mathrm{d}x=\\frac{1}{2}x^{2}y^{2}-\\frac{1}{2}x^{2}-\\frac{1}{2}y^{2}\r$$$$\r带入得：\\frac{1}{2}x^{2}y^{2}-\\frac{1}{2}x^{2}-\\frac{1}{2}y^{2}=-\\frac{1}{4}\r$$ 题目来源\n$$\r\\frac{\\mathrm{d}y}{\\mathrm{d}x}=-\\frac{1}{4}y^2lnx\\\\\r-\\frac{1}{y^2}\\mathrm{d}y=\\frac{1}{4}lnx\\mathrm{d}x\\\\\r\\frac{1}{y}=\\frac{1}{4}x(lnx-1)+C\\\\\ry=\\frac{4}{x(lnx-1)+C}\r$$$$\r(y^3+y)\\mathrm{d}x=-x(y^2-1)\\mathrm{d}y\\\\\r-\\frac{1}{x}\\mathrm{d}x=\\frac{y^2-1}{y^3+y}\\mathrm{d}y=\\frac{y^2+1-2}{y^3+y}\\mathrm{d}y=(\\frac{1}{y}-\\frac{2}{y(y^2+1)})\\mathrm{d}y\\\\\r-\\frac{1}{x}\\mathrm{d}x=(-\\frac{1}{y}+\\frac{2y}{y^2+1})\\mathrm{d}y\\\\\r\\frac{y}{y^2+1}=x+C\r$$$$\ry=ue^{sinx},\\frac{\\mathrm{d}y}{\\mathrm{d}x}=u'e^{sinx}+ue^{sinx}cosx\\\\\r代入u'=3x,u=\\frac{3}{2}x^2+C\\\\\ry=(\\frac{3}{2}x^2+C)e^{sinx},C=4\\\\\ry=(\\frac{3}{2}x^2+4)e^{sinx}\r$$$$\r\\frac{\\mathrm{d}(\\frac{1}{y})}{\\mathrm{d}x}+2x\\frac{1}{y}=e^{-x^2}cosx\\\\\rz=\\frac{1}{y},\\frac{\\mathrm{d}z}{\\mathrm{d}x}+2xz=e^{-x^2}cosx\\\\\rz=ue^{-x^2},\\frac{\\mathrm{d}z}{\\mathrm{d}x}=u'e^{-x^2}-2uxe^{x^2},u'e^{-x^2}=e^{-x^2}cosx,u'=cosx\\\\\ru=sinx+C,\\therefore z=\\frac{1}{y}=(sinx+C)e^{-x^2},y=\\frac{1}{(sinx+C)e^{-x^2}}\r$$$$\r懒得写了，y'=\\frac{1}{2}e^{2y}+C_1,(C_1=0)\\\\\r-\\frac{1}{2}e^{-2y}=\\frac{1}{2}x+C_2(C_2=-\\frac{1}{2})\\\\\ry=-\\frac{1}{2}ln(1-x)\r$$","date":"2025-01-23T00:00:00Z","image":"http://localhost:1313/hh.jpg","permalink":"http://localhost:1313/p/ode%E5%85%AC%E5%BC%8F/","title":"ODE公式"},{"content":" 参考西瓜书和某大佬blog，忘记博客地址了（手动狗头）\n拉格朗日数乘法： $$ 对于m个等式以及n个不等式的约束，h_i(x)=0(i=1,2,...m),g_j(x)\\le0(j=1,2,...,n)\\\\简记为:\\\\ min\\ f(x)\\\\s.t.\\ h_i(x)=0(i=1,2,...m)\\\\g_j(x)\\le0(j=1,2,...,n)\\\\ $$KKT条件 $$ 对于g_j(x)\\le0，我们引入KKT条件(Karush-Kuhn-Tucker) $$$$ \\lambda_j\\ge0,g_j(x)\\le0,\\lambda_j g_j(x)=0 $$对偶条件 这个问题（或者说原始的带约束的形式）称作 primal problem。如果你看过之前关于 SVM 的指导，那么肯定就知道了，相对应的还有一个 dual problem，其形式非常类似，只是把 min 和 max 交换了一下：\n$$ \\max_{\\lambda \\geq 0, \\nu} \\min_{x} L(x, \\lambda, \\nu) $$交换之后的 dual problem 和原来的 primal problem 并不相等，直观地，我们可以这样理解：解子问题的精力分布得最精细的那部分要明显。当然这是很不严格的说法，而是出于字数的限制以便简单说明，所以我们还是来看一下数学描述。和刚才的 \\( x(z) \\) 类似，我们也用一个记号来表示内层的这个函数，记：\n$$ g(\\lambda, \\nu) = \\min_{x} L(x, \\lambda, \\nu) $$并称 \\( g(\\lambda, \\nu) \\) 为 Lagrange dual function （不要和 \\( L \\) 的 Lagrangian 混淆了）。\\( g \\) 有一个很好的性质就是它是 primal problem 的一个下界。换句话说，如果 primal problem 的最小值记为 \\( p^* \\)，那么对于所有的 \\( \\lambda \\geq 0 \\)，我们有：\n$$ g(\\lambda, \\nu) \\leq p^* $$因为对于极值点（实际上包括所有满足约束条件的点）\\( x^* \\)，注意到 \\( \\lambda \\geq 0 \\)，我们总是有：\n$$ \\sum_{i=1}^m \\lambda_i f_i(x^*) + \\sum_{i=1}^p \\nu_i h_i(x^*) \\leq 0 $$因此\n$$ L(x^*, \\lambda, \\nu) = f_0(x^*) + \\sum_{i=1}^m \\lambda_i f_i(x^*) + \\sum_{i=1}^p \\nu_i h_i(x^*) \\leq f_0(x^*) = p^* $$于是\n$$ g(\\lambda, \\nu) = \\min_{x} L(x, \\lambda, \\nu) \\leq L(x^*, \\lambda, \\nu) \\leq f_0(x^*) = p^* $$这样一来就确定了 \\( g \\) 的下界性质，于是\n$$ \\max_{\\lambda \\geq 0, \\nu} g(\\lambda, \\nu) $$ 实际上就是最大的下界。这是很自然的，因为得到了下界之后，我们自然就希望得到最好的下界，也就是最大的那一个——因为它离我们要逼近的值最近呀。记 dual problem 的最优值为 \\( d^* \\) 的话，根据上面的推导，我们就得到了如下性质：\n$$ d^* \\leq p^* $$","date":"2025-01-21T00:00:00Z","image":"http://localhost:1313/sas.jpg","permalink":"http://localhost:1313/p/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%95%B0%E4%B9%98%E6%B3%95/","title":"拉格朗日数乘法"},{"content":"​\t关于这首歌的背景，它是Cobain写给当时的女朋友Tracey Marander的。当时，Cobain被家人赶出了家门，基本过上了流浪汉的生活。是她女朋友支持了他的音乐事业，他生活在Marander的公寓里，他并没有工作，而是花大部分时间在写歌、画画和睡觉，而Marander则需要工作来支付租金和生活费。\n​\t这首歌和他的专辑《bleach》的别的歌风格大大不同，《bleach》充斥了沙哑的嘶吼和对社会的叛逆，about a girl旋律轻快，基本是一个人的倾诉和爱慕，据科特所说这是为了表达对Marander的感谢，但是有趣的是当时Marander似乎不知道。\nAbout a girl I need an easy friend\n我想要一个可以交往的朋友\nI do\u0026hellip; with an ear to lend\n我会倾听你的想法\nI do\u0026hellip; think you fit this shoe\n我觉得\u0026hellip;你是合适的人\nI do\u0026hellip; but you have a clue\n但是，你知道吗\nI\u0026rsquo;ll take advantage while\n我会把握好机会\nYou hang me out to dry\n当你把我甩在一边\nBut I can\u0026rsquo;t see you every night\n但是我不能每晚都如愿见到你\n（for） free I do I\u0026rsquo;m standing in your line\n我会站在你那边\nI do\u0026hellip; hope you have the time\n我真的\u0026hellip;希望你有时间\nI do\u0026hellip; pick a number too （to or two）\n我会要到你的电话\nI do\u0026hellip; keep a date with you\n我会和你约会\nI\u0026rsquo;ll take advantage while\n我会把握好机会\nYou hang me out to dry\n当你把我甩在一边\nBut I can\u0026rsquo;t see you every night\n但是我不能每晚都如愿见到你\n（for） free I do\n","date":"2025-01-20T00:00:00Z","image":"http://localhost:1313/N.jpg","permalink":"http://localhost:1313/p/about-a-girl/","title":"About a girl"}]